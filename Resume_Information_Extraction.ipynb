{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/layafakher/Resume_Information_Extraction/blob/main/Resume_Information_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKHZCmWddHiP"
      },
      "source": [
        "#Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Hyqcslm7AT",
        "outputId": "9cdf69e8-0bcc-4a05-a745-35298fbe09fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBCMmrn4pi7K",
        "outputId": "5666faa6-160c-417b-ccbf-25d27ac8786e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from PyPDF2 import PdfReader, PdfFileWriter, PdfFileMerger  # Library for working with PDF files\n",
        "import os  # Library for interacting with the operating system\n",
        "import pandas as pd  # Library for working with data in tabular form\n",
        "import re  # Library for regular expressions\n",
        "from nltk.corpus import stopwords  # Library for stopwords\n",
        "import re  # Library for regular expressions\n",
        "import nltk  # Natural Language Processing library\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize  # Natural Language Processing functions\n",
        "from nltk.tree import Tree  # nltk tree structure\n",
        "import spacy  # Natural Language Processing library\n",
        "nltk.download('stopwords')  # Download necessary data from the nltk server\n",
        "from nltk.corpus import stopwords  # Library for stopwords\n",
        "nltk.download('punkt')  # Download necessary data from the nltk server\n",
        "nltk.download('averaged_perceptron_tagger')  # Download necessary data from the nltk server\n",
        "nltk.download('maxent_ne_chunker')  # Download necessary data from the nltk server\n",
        "nltk.download('words')  # Download necessary data from the nltk server\n",
        "import spacy  # Natural Language Processing library\n",
        "import en_core_web_sm  # Language model for spacy\n",
        "from spacy.matcher import Matcher  # Matcher for spacy\n",
        "import pandas as pd  # Library for working with data in tabular form\n",
        "import spacy  # Natural Language Processing library\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize  # Natural Language Processing functions\n",
        "from nltk.tree import Tree  # nltk tree structure\n",
        "nlp = spacy.load('en_core_web_sm')  # Load spacy language model\n",
        "from collections import defaultdict  # Library for default dictionaries\n",
        "from nltk.corpus import stopwords  # Library for stopwords\n",
        "from nltk.tokenize import word_tokenize  # Functions for tokenizing sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMJIbsl9wIqt",
        "outputId": "f0cb2769-9139-4d8f-991b-d4fca4ff6cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh6i3ojSwMiG"
      },
      "outputs": [],
      "source": [
        "SST_HOME='drive/My Drive/Sample Data Resume/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEfWwM7EdCQ3"
      },
      "source": [
        "#Converting PDFs to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAzZSbuUrQGr"
      },
      "outputs": [],
      "source": [
        "def pdftotext(path):\n",
        "    \"\"\"\n",
        "    Function to convert a PDF file to text.\n",
        "\n",
        "    :param path: Path of the PDF file\n",
        "    :return: Extracted text from the PDF\n",
        "    \"\"\"\n",
        "    pdfFileObj = open(path, 'rb')\n",
        "    pdfFileReader = PdfReader(pdfFileObj)\n",
        "    num_pages = len(pdfFileReader.pages)\n",
        "    currentPageNumber = 0\n",
        "    text = ''\n",
        "    while(currentPageNumber < num_pages ):\n",
        "        pdfPage = pdfFileReader.pages[num_pages-1]\n",
        "        text = text + pdfPage.extract_text()\n",
        "        text = text.encode('utf-8', 'ignore').decode('utf-8')\n",
        "        currentPageNumber += 1\n",
        "\n",
        "    return (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LSiTvX7cVJF",
        "outputId": "7738dd54-92c3-42be-bee9-ea2f0a098f1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/Sample Data Resume/CV-LayaFakher.pdf',\n",
              " 'drive/My Drive/Sample Data Resume/10985403.pdf',\n",
              " 'drive/My Drive/Sample Data Resume/11890896.pdf',\n",
              " 'drive/My Drive/Sample Data Resume/12377803.pdf',\n",
              " 'drive/My Drive/Sample Data Resume/12230301.pdf',\n",
              " 'drive/My Drive/Sample Data Resume/11704150.pdf',\n",
              " 'drive/My Drive/Sample Data Resume/11653906.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "PDFs = []\n",
        "for filename in os.listdir(SST_HOME):\n",
        "    f = os.path.join(SST_HOME, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        PDFs.append(f)\n",
        "PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tjYQ7DbgmYn"
      },
      "source": [
        "#Extracting Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhv5iePmgkzv"
      },
      "outputs": [],
      "source": [
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5kUMRZ8pFnA"
      },
      "outputs": [],
      "source": [
        "def match_name(name):\n",
        "    # Loading the English language model for Spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    patterns = [\n",
        "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],  # First name and Last name\n",
        "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],  # First name, Middle name, and Last name\n",
        "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]  # First name, Middle name, Middle name, and Last name\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matcher.add('NAME', patterns=[pattern])\n",
        "\n",
        "    doc = nlp(name)\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvd7ob84jWb4"
      },
      "outputs": [],
      "source": [
        "def extract_names(text):\n",
        "  names = []\n",
        "  spacy_parser = nlp(text)\n",
        "  for entity in spacy_parser.ents:\n",
        "      if entity.label_ ==  \"PERSON\" and match_name(entity.text):\n",
        "          return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMk31_ZK3wap"
      },
      "outputs": [],
      "source": [
        "def extract_email_from_resume(string):\n",
        "  # Regular expression for extracting email addresses\n",
        "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "    return r.findall(string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEYvd8bB34vi"
      },
      "outputs": [],
      "source": [
        "def extract_skills(text):\n",
        "    skills = defaultdict(int)\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    for word in words:\n",
        "        is_skill = True\n",
        "        spacy_parser = nlp(word)\n",
        "        for entity in spacy_parser.ents:\n",
        "            if entity.label_  in [\"DATE\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"GPE\", \"ORG\", \"FAC\" , \"NORP\"]:\n",
        "                is_skill = False\n",
        "        if word not in stop_words and is_skill:\n",
        "            skills[word] += 1\n",
        "\n",
        "    # Extracting top 20 skills\n",
        "    top_skills = sorted(skills, key=skills.get, reverse=True)[:20]\n",
        "    return top_skills\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksuLBUFUWjfj"
      },
      "source": [
        "#Resume Information Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am0HyP52rBEx"
      },
      "outputs": [],
      "source": [
        "def resume_information_extractor(resume_paths):\n",
        "    for resume_path in resume_paths:\n",
        "        text = pdftotext(resume_path)\n",
        "\n",
        "        print(\"Resume:\", resume_path)\n",
        "\n",
        "        name = extract_names(text)\n",
        "        if name:\n",
        "            print(\"Name:\", name)\n",
        "        else:\n",
        "            print(\"Name not found\")\n",
        "\n",
        "\n",
        "        email = extract_email_from_resume(text)\n",
        "        if email:\n",
        "            print(\"Email:\", email)\n",
        "        else:\n",
        "            print(\"Email not found\")\n",
        "\n",
        "        extracted_skills = extract_skills(text)\n",
        "        if extracted_skills:\n",
        "            print(\"Skills:\", extracted_skills)\n",
        "        else:\n",
        "            print(\"No skills found\")\n",
        "\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_information_extractor(PDFs)"
      ],
      "metadata": {
        "id": "hy6yvejX9pso"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMov72HUQkrI785voEj3CFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}